{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chosen example: \n",
    "\n",
    "VAE: clean-gull-718 (Run id: a75dc5ac2d4e443585bc79d76efa02ac)\n",
    "    beta_start = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/../..')\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "from filepaths import filepath_from_ml_artifacts_uri\n",
    "from analysis.iclr.plotting import settings, fig_size, cb_line_cycler, cb_line_cycler_solid, cb_marker_cycler, savefig\n",
    "from plot_hdf5_dataset import plot_dataset\n",
    "from iclr_paper_path import get_paper_path\n",
    "import matplotlib\n",
    "settings()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import omegaconf\n",
    "import torch\n",
    "from networks.src.load_data import load_dataset_and_config, make_stacked_dataset\n",
    "from networks.pels_vae_linear.vae_architecture import VAE\n",
    "\n",
    "from networks.src.kullback_leibler import kullback_leibler, count_populated_dimensions\n",
    "# from networks.pels_vae_linear.vae_architecture import VAE, loss_function\n",
    "from analysis.C03_Methods.data.clean_gull_718.artifacts.vae_architecture import VAE, loss_function\n",
    "from config import train_test_config_class\n",
    "from networks.src.load_data import load_dataset_and_config, make_stacked_dataset\n",
    "from networks.src.early_stopping import EarlyStopping\n",
    "from networks.src.capacity_scheduler import capacity_scheduler as CapacityScheduler\n",
    "from utils.hydra_mlflow_decorator import log_hydra_to_mlflow\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\projects\\balanced_neural_odes_internal\\code\\python\n",
      "run this cell only once\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../../..')\n",
    "print(os.getcwd())\n",
    "\n",
    "print('run this cell only once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vae = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_vae:\n",
    "    path_dataset_vae = filepath_from_ml_artifacts_uri('mlflow-artifacts:/595842590057669767/a75dc5ac2d4e443585bc79d76efa02ac/artifacts/dataset.hdf5')\n",
    "    path_model_vae = filepath_from_ml_artifacts_uri('mlflow-artifacts:/595842590057669767/a75dc5ac2d4e443585bc79d76efa02ac/artifacts/model.pt')\n",
    "else: \n",
    "    path_dataset_vae = 'analysis/C03_Methods/data/rare_moth_313/artifacts/dataset.hdf5'\n",
    "    path_model_vae = 'analysis/C03_Methods/data/rare_moth_313/artifacts/model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['common_test', 'common_validation', 'outputs_names', 'parameters_names', 'states_der_names', 'states_names', 'test', 'time', 'train', 'validation']>\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "dataset_result = h5py.File(path_dataset_vae, 'r')\n",
    "print(dataset_result.keys())\n",
    "kl_test = dataset_result['common_test']['kl_loss_raw'][:]\n",
    "\n",
    "use_logvar = False\n",
    "# determine order of dims\n",
    "kl_test_mean_per_dim = np.mean(kl_test, axis=0)\n",
    "print(kl_test_mean_per_dim.shape)\n",
    "ordered_dims = np.argsort(kl_test_mean_per_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (timeseries_normalization): NormalizationLayerTimeSeries()\n",
       "  (Regressor): Regressor(\n",
       "    (normalization): NormalizationLayer1D(num_features=14)\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (Encoder): Encoder(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=1685, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=2048, out_features=1685, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if test_vae:\n",
    "    _path =  filepath_from_ml_artifacts_uri('mlflow-artifacts:/595842590057669767/a75dc5ac2d4e443585bc79d76efa02ac/artifacts/.hydra/config.yaml')\n",
    "    cfg = omegaconf.OmegaConf.load(_path)\n",
    "else:\n",
    "    cfg = omegaconf.OmegaConf.load('analysis/C03_Methods/data/rare_moth_313/artifacts/.hydra/config.yaml')\n",
    "\n",
    "# load dataset and config\n",
    "dataset, dataset_config = load_dataset_and_config(cfg)\n",
    "\n",
    "# make train and test torch tensor datasets\n",
    "train_dataset = make_stacked_dataset(dataset_config, dataset, 'train')\n",
    "test_dataset = make_stacked_dataset(dataset_config, dataset, 'test')\n",
    "validation_dataset = make_stacked_dataset(dataset_config, dataset, 'validation')\n",
    "common_test_dataset = make_stacked_dataset(dataset_config, dataset, 'common_test')\n",
    "\n",
    "# initialize data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cfg.nn_model.training.batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg.nn_model.training.batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=cfg.nn_model.training.batch_size, shuffle=True)\n",
    "common_test_loader = torch.utils.data.DataLoader(common_test_dataset, batch_size=cfg.nn_model.training.batch_size, shuffle=True)\n",
    "\n",
    "# initialize model\n",
    "model = VAE(\n",
    "    n_states=dataset['train']['states'].shape[1],\n",
    "    n_outputs=dataset['train']['outputs'].shape[1],\n",
    "    seq_len=dataset['train']['states'].shape[2],\n",
    "    parameter_dim=dataset['train']['parameters'].shape[1],\n",
    "    hidden_dim=cfg.nn_model.network.linear_hidden_dim,\n",
    "    bottleneck_dim=cfg.nn_model.network.n_latent,\n",
    "    activation=eval(cfg.nn_model.network.activation),\n",
    "    n_layers=cfg.nn_model.network.n_linear_layers,\n",
    "    params_to_decoder=cfg.nn_model.network.params_to_decoder,\n",
    ")\n",
    "\n",
    "# load model\n",
    "model.load_state_dict(torch.load(path_model_vae))\n",
    "model.eval()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define one model and loss evaluation\n",
    "def model_and_loss_evaluation(model, states, outputs, parameters, train=True, n_passes: int = 1, return_model_outputs: bool = False, active_dims = None):\n",
    "    # active_dims is a torch tensor of shape (n_latent) with boolean values\n",
    "    x, x_hat, states_hat, outputs_hat, mu, logvar, mu_hat, logvar_hat, normed_values = model(states, outputs, parameters, train=train, predict = False, n_passes=n_passes, active_dims=active_dims)\n",
    "    \n",
    "    loss, mse_loss, kl_loss, regressor_loss = loss_function(\n",
    "                normed_values['x'], normed_values['x_hat'], mu, mu_hat, \n",
    "                logvar, logvar_hat, \n",
    "                beta=cfg.nn_model.training.beta_start, \n",
    "                gamma=cfg.nn_model.training.gamma,\n",
    "                capacity= None,\n",
    "    )\n",
    "    ret_val = {\n",
    "        'loss': loss,\n",
    "        'mse_loss': mse_loss,\n",
    "        'kl_loss': kl_loss,\n",
    "        'regressor_loss': regressor_loss,\n",
    "        'populated_dims': count_populated_dimensions(mu, logvar, cfg.nn_model.training.count_populated_dimensions_threshold)[0]\n",
    "    }\n",
    "    if return_model_outputs:\n",
    "        # losses per dim\n",
    "        _, mse_loss_raw, kl_loss_raw, regressor_loss_raw = loss_function(\n",
    "                x, x_hat, mu, mu_hat, \n",
    "                logvar, logvar_hat, \n",
    "                beta=cfg.nn_model.training.beta_start, \n",
    "                gamma=cfg.nn_model.training.gamma,\n",
    "                capacity= None,\n",
    "                reduce=False\n",
    "                )   \n",
    "        model_outputs = {\n",
    "            'mse_loss_raw': mse_loss_raw,\n",
    "            'kl_loss_raw': kl_loss_raw,\n",
    "            'regressor_loss_raw': regressor_loss_raw,\n",
    "            'states_hat': states_hat,\n",
    "            'outputs_hat': outputs_hat,\n",
    "            'mu': mu,\n",
    "            'logvar': logvar,\n",
    "            'mu_hat': mu_hat,\n",
    "            'logvar_hat': logvar_hat,\n",
    "        }\n",
    "    if not train:\n",
    "        # call value.item() for each value in return_value\n",
    "        ret_val = dict({key: value.item() for key, value in ret_val.items()})\n",
    "        if return_model_outputs:\n",
    "            model_outputs = dict({key: value.cpu().detach().numpy() for key, value in model_outputs.items()})\n",
    "    return ret_val if not return_model_outputs else (ret_val, model_outputs)\n",
    "    \n",
    "def get_model_inputs(data_loader: torch.utils.data.DataLoader, data: dict = None):\n",
    "    if data_loader is None:\n",
    "        assert data is not None, 'Either data_loader or data must be not None'\n",
    "    else:\n",
    "        data = next(iter(data_loader))\n",
    "    # get data from data loader\n",
    "    states = data['states'].to(device)\n",
    "    outputs = data['outputs'].to(device)\n",
    "    parameters = data['parameters'].to(device)\n",
    "    return states, outputs, parameters\n",
    "\n",
    "def test_or_validate_one_epoch(model, _data_loader, n_passes: int = 1, all_batches: bool = False,\n",
    "                                return_model_outputs: bool = False, active_dims = None):\n",
    "    \n",
    "    model.eval()\n",
    "    # make sure that the data loader is not shuffled by initializing a new data loader\n",
    "    if all_batches:\n",
    "        data_loader = torch.utils.data.DataLoader(_data_loader.dataset, batch_size=_data_loader.batch_size, shuffle=False)\n",
    "    else:\n",
    "        data_loader = _data_loader\n",
    "    _ret_vals = []\n",
    "    _model_outputs = []\n",
    "    for step, data in enumerate(data_loader):\n",
    "        states, outputs, parameters = get_model_inputs(data_loader=None, data=data)\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            ret_vals, model_outputs = model_and_loss_evaluation(model, states, outputs, parameters, train=False, n_passes=n_passes, return_model_outputs=True, active_dims=active_dims)\n",
    "        _ret_vals.append(ret_vals)\n",
    "        _model_outputs.append(model_outputs)\n",
    "        if all_batches is False:\n",
    "            break\n",
    "    # average over all calls\n",
    "    if all_batches:\n",
    "        ret_vals = {}\n",
    "        for key in _ret_vals[0].keys():\n",
    "            ret_vals[key] = sum([_ret_val[key] for _ret_val in _ret_vals]) / len(_ret_vals)\n",
    "    else:\n",
    "        ret_vals = _ret_vals[0]\n",
    "    # make one tensor from all model outputs\n",
    "    if return_model_outputs:\n",
    "        model_outputs = {key: np.concatenate([_batch_output[key] for _batch_output in _model_outputs], axis=0) for key in _model_outputs[0].keys()}\n",
    "    return ret_vals if not return_model_outputs else (ret_vals, model_outputs)\n",
    "\n",
    "def append_context_to_dict_keys(dictionary: dict, context: str):\n",
    "    return dict({'{}_{}'.format(key, context): value for key, value in dictionary.items()})\n",
    "\n",
    "ret_vals_active_dims = []\n",
    "for dim_run in range(kl_test.shape[1]+1):\n",
    "    # make mask with active dims\n",
    "    active_dims = torch.zeros(kl_test.shape[1], dtype=torch.bool)\n",
    "    active_dims[ordered_dims[:dim_run]] = True\n",
    "\n",
    "    # add model outputs to dataset\n",
    "    for context, dataloader in zip(['common_test'], [common_test_loader]):\n",
    "        ret_vals, model_outputs = test_or_validate_one_epoch(model, dataloader, n_passes=cfg.nn_model.training.n_passes_test, all_batches=True, return_model_outputs=True, active_dims=active_dims)\n",
    "        ret_vals_active_dims.append(ret_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_vals_active_dims = pd.DataFrame(ret_vals_active_dims)\n",
    "if test_vae:\n",
    "    ret_vals_active_dims.to_csv('analysis/C03_Methods/data/clean_gull_718/artifacts/ret_vals_active_dims.csv')\n",
    "else:\n",
    "    ret_vals_active_dims.to_csv('analysis/C03_Methods/data/rare_moth_313/artifacts/ret_vals_active_dims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(test_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mse_loss</th>\n",
       "      <th>kl_loss</th>\n",
       "      <th>regressor_loss</th>\n",
       "      <th>populated_dims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.861725</td>\n",
       "      <td>0.846800</td>\n",
       "      <td>13.786067</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.367240</td>\n",
       "      <td>0.352316</td>\n",
       "      <td>13.786067</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.226983</td>\n",
       "      <td>0.212058</td>\n",
       "      <td>13.786067</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183194</td>\n",
       "      <td>0.168269</td>\n",
       "      <td>13.786067</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138693</td>\n",
       "      <td>0.123768</td>\n",
       "      <td>13.786067</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  mse_loss    kl_loss  regressor_loss  populated_dims\n",
       "0  0.861725  0.846800  13.786067        0.001139             5.0\n",
       "1  0.367240  0.352316  13.786067        0.001139             5.0\n",
       "2  0.226983  0.212058  13.786067        0.001139             5.0\n",
       "3  0.183194  0.168269  13.786067        0.001139             5.0\n",
       "4  0.138693  0.123768  13.786067        0.001139             5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "if test_vae:\n",
    "    ret_vals_active_dims = pd.read_csv('analysis/C03_Methods/data/clean_gull_718/artifacts/ret_vals_active_dims.csv', index_col=0)\n",
    "else:\n",
    "    ret_vals_active_dims = pd.read_csv('analysis/C03_Methods/data/rare_moth_313/artifacts/ret_vals_active_dims.csv', index_col=0)\n",
    "# reverse order of index\n",
    "ret_vals_active_dims = ret_vals_active_dims.iloc[::-1]\n",
    "# initiate index new\n",
    "ret_vals_active_dims.index = np.arange(ret_vals_active_dims.shape[0])\n",
    "ret_vals_active_dims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.846800\n",
       "1    0.352316\n",
       "2    0.212058\n",
       "3    0.168269\n",
       "4    0.123768\n",
       "Name: mse_loss, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_vals_active_dims['mse_loss'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding path that contains \"code\", then going up one level to find \"LaTeX\"\n",
      "e:\\projects\\balanced_neural_odes_internal\\code\n",
      "e:\\projects\\balanced_neural_odes_internal\n",
      "E:\\projects\\balanced_neural_odes_internal\\LaTeX\\ICLR 2025 Template\\figures\\VAE\\active_dims_sweep_VAE.png\n",
      "E:\\projects\\balanced_neural_odes_internal\\LaTeX\\ICLR 2025 Template\\figures\\VAE\\active_dims_sweep_VAE.svg\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 275.054x88 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make plot\n",
    "fig, ax = plt.subplots(1,1, figsize=fig_size(0.88, n_figs_per_width=2), sharey=True, constrained_layout=True)\n",
    "ax = [ax]\n",
    "\n",
    "# plot with reversed order\n",
    "x = np.arange(0, kl_test.shape[1]+1)\n",
    "ax[0].step(x, ret_vals_active_dims['mse_loss'], label='MSE loss', color = '#E69F00', lw = 1.0, zorder = 1, where='post')\n",
    "\n",
    "def add_point(i, ax_i = 0, rotate = 20):\n",
    "    if i == 0:\n",
    "        i_x = 1\n",
    "    else:\n",
    "        i_x = i\n",
    "    # put a point at x = 5 with the value of the y axis\n",
    "    ax[ax_i].scatter(i_x, ret_vals_active_dims['mse_loss'][i], color = 'black', marker = 'o', s = 2)\n",
    "    # add text\n",
    "    _text = '{:.2f} @{}'.format(ret_vals_active_dims['mse_loss'][i],i)\n",
    "    ax[ax_i].text(i_x, ret_vals_active_dims['mse_loss'][i], _text, fontsize = 6, va = 'bottom', ha = 'left', rotation = rotate)\n",
    "\n",
    "add_point(0, rotate=0)\n",
    "add_point(1)\n",
    "add_point(2)\n",
    "add_point(3)\n",
    "add_point(5)\n",
    "# add_point(16)\n",
    "add_point(32)\n",
    "# add_point(64)\n",
    "add_point(128)\n",
    "# add_point(256)\n",
    "add_point(512)\n",
    "\n",
    "# set x axis to log scale\n",
    "ax[0].set_xscale('log')\n",
    "\n",
    "# # ax.set_xscale('log')\n",
    "# ax[1].plot(ret_vals_active_dims['mse_loss'][:20], label='MSE loss', color = 'black', lw = 1.0)\n",
    "# if test_vae is False:\n",
    "#     add_point(0, ax_i = 1)\n",
    "# add_point(5, ax_i = 1)\n",
    "# add_point(4, ax_i = 1)\n",
    "# add_point(3, ax_i = 1)\n",
    "# add_point(2, ax_i = 1)\n",
    "# add_point(1, ax_i = 1)\n",
    "# add_point(10, ax_i = 1)\n",
    "# add_point(17, ax_i = 1)\n",
    "\n",
    "# ax[1].set_xticks(np.arange(0,20,2))\n",
    "\n",
    "ax[0].set_ylabel('MSE')\n",
    "# ax[1].set_ylabel('MSE loss')\n",
    "ax[0].set_xlabel('\\# active latent dimensions')\n",
    "\n",
    "ax[0].set_ylim([0.0, 1.1])\n",
    "ax[0].set_xlim([0.8,512])\n",
    "\n",
    "# save figure\n",
    "if test_vae:\n",
    "    savefig(fig, get_paper_path('VAE', 'active_dims_sweep_VAE'))\n",
    "else:\n",
    "    savefig(fig, get_paper_path('VAE', 'active_dims_sweep_PELS_VAE'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th_ak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
