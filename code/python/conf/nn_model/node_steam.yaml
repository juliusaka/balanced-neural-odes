defaults:
  - neural_ode_base
  - _self_

network:
  n_linear_layers: 4
  linear_hidden_dim: 128
  hidden_dim_output_nn: 128
  n_layers_output_nn: 4
  activation: torch.nn.ELU
  controls_to_output_nn: true
  
training:
  pre_train: false
  load_pretrained_model: false
  load_trained_model_for_test: false
  batch_size_test: 256

  path_pretrained_model: ''
  path_trained_model: ''
  pre_trained_model_seq_len: null

  lr_start_override: 1e-5
  weight_decay_override: 1e-8
  early_stopping_threshold_override: 0.00001
  early_stopping_threshold_mode_override: abs

  reload_optimizer_override: true
  use_adjoint_override: false
  solver_rtol_override: 1e-3
  solver_atol_override: 1e-4
  solver_norm_override: mixed
  # beta1_adam_override: 0.8
  # beta2_adam_override: 0.99
  clip_grad_norm_override: 1.0

  # pre_training:
  #   batch_size: 100
  #   lr_start: 1e-3
  #   max_epochs: 4000
  #   early_stopping_patience: 100
  #   weight_decay: 1e-5
  #   method: collocation
  main_training:
  # get trainable model
  - batch_size: 800
    solver: euler
    lr_start: 1e-3
    max_epochs: 500
    early_stopping_patience: 10
    load_seq_len: null
    seq_len_train: 10
    evaluate_at_control_times: false
    break_after_loss_of: 0.10
  # pre training with 30 steps
  - batch_size: 512 # besser 256?
    solver: rk4
    lr_start: 1e-3
    max_epochs: 4000 # 2000?
    early_stopping_patience: 400
    load_seq_len: null
    seq_len_train: 80
    evaluate_at_control_times: false
    seq_len_increase_in_batches: 1200
    seq_len_increase_abort_after_n_stable_epochs: 1000
    # break_after_loss_of: 0.1

  # # increase seq len to 250
  - batch_size: 128
    solver: rk4
    lr_start: 1e-3
    max_epochs: 2000
    early_stopping_patience: 100
    seq_len_train: 250 
    seq_len_increase_in_batches: 1200
    seq_len_increase_abort_after_n_stable_epochs: 1000
    evaluate_at_control_times: false